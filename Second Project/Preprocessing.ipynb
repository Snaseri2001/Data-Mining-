{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d7ea7d1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "id": "3d7ea7d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7c6b03d"
      },
      "source": [
        "# Data Preprocessing"
      ],
      "id": "c7c6b03d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0642354e"
      },
      "source": [
        "Firstly we split x and y of location ."
      ],
      "id": "0642354e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2fbd42e8",
        "scrolled": true,
        "outputId": "dadf06a0-9109-4bcc-b770-51f0a61bec0b"
      },
      "outputs": [
        {
          "ename": "ParserError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9c990ed02492>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# at first we get the data set using pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mCrime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Raw_Dataset.csv\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'windows-1254'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnew_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mCrime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 17 fields in line 5796, saw 18\n"
          ]
        }
      ],
      "source": [
        "# at first we get the data set using pandas\n",
        "Crime = pd.read_csv(\"/content/Raw_Dataset.csv\" , encoding='windows-1254')\n",
        "new_dataset = pd.DataFrame()\n",
        "Crime.head()\n"
      ],
      "id": "2fbd42e8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d7dc00c2"
      },
      "outputs": [],
      "source": [],
      "id": "d7dc00c2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5a86c04"
      },
      "source": [
        "Here we are going to work on \"OFFENSE_DESCRIPTION\" at first we fill in the null data , then we eliminate \" ?.!/;:\" and finally we convert it to lower case."
      ],
      "id": "a5a86c04"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cf9899f"
      },
      "outputs": [],
      "source": [
        "# Get Description And Cleaning & Convert To Lower Case\n",
        "Description=Crime['OFFENSE_DESCRIPTION']\n",
        "\n",
        "for key in Description.isnull():\n",
        "    if key == True:\n",
        "        print(\"NULL\")\n",
        "\n",
        "des=[]\n",
        "for letter in Description:\n",
        "    for char in letter:\n",
        "        if char in \" ?.!/;:\":\n",
        "           letter.replace(char,'')\n",
        "    des.append(letter.lower().strip().replace(' - ','-'))\n",
        "new_dataset[\"OFFENSE_DESCRIPTION\"] = pd.DataFrame(des)"
      ],
      "id": "6cf9899f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48df9d15"
      },
      "source": [
        "we maintain the \"DISTRICT\" , \"DAY_OF_WEEK\"  , \"YEAR\" , \"HOUR\" and \"MONTH\" features without any change ."
      ],
      "id": "48df9d15"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ee29be8"
      },
      "outputs": [],
      "source": [
        "new_dataset[\"DISTRICT\"] = Crime[\"DISTRICT\"]\n",
        "new_dataset[\"DAY_OF_WEEK\"] = LabelEncoder().fit_transform(Crime[\"DAY_OF_WEEK\"]) \n",
        "new_dataset[\"MONTH\"] = Crime[\"MONTH\"]\n",
        "new_dataset[\"YEAR\"] = Crime[\"YEAR\"]\n",
        "new_dataset[\"HOUR\"] = Crime[\"HOUR\"]"
      ],
      "id": "3ee29be8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ba7986c"
      },
      "source": [
        "here we split the date and time of occurrence ."
      ],
      "id": "0ba7986c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ac9058e"
      },
      "outputs": [],
      "source": [
        "OCCURRED_DATE=Crime[\"OCCURRED_ON_DATE\"]\n",
        "date=[]\n",
        "time=[]\n",
        "for i in OCCURRED_DATE:\n",
        "    date.append(i.split(\" \")[0])\n",
        "    time.append(i.split(\" \")[1])"
      ],
      "id": "6ac9058e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cca1b12"
      },
      "outputs": [],
      "source": [
        "# our time consists of hour , minute and second but we don not need to \n",
        "# know the sceond so we eliminate it and maintain hour and minute \n",
        "new_time=[]\n",
        "for t in time :\n",
        "    split_timwe=t.split(':')\n",
        "    hours=split_timwe[0]\n",
        "    minutes=split_timwe[1]\n",
        "    new_time.append(hours+\":\"+minutes)\n",
        "# we alse change the form of our dates \n",
        "desired_date = '2018-08-30'\n",
        "desired_list = [x for x in date if x >= desired_date]"
      ],
      "id": "8cca1b12"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e3bba1b"
      },
      "outputs": [],
      "source": [
        "new_dataset[\"date\"] = pd.DataFrame(desired_list)\n",
        "new_dataset[\"time\"] = pd.DataFrame(new_time)"
      ],
      "id": "4e3bba1b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f6510af"
      },
      "outputs": [],
      "source": [
        "# Get UCR PART - And Encoding Them \n",
        "UCR_PART=Crime[\"UCR_PART\"]\n",
        "for item in range(len(Crime)) :\n",
        "  if pd.isna(Crime[\"UCR_PART\"][item]) :\n",
        "    Crime[\"UCR_PART\"].iloc[item] = \"Part zero\"\n",
        "\n",
        "new_dataset[\"UCR_PART\"] = LabelEncoder().fit_transform(Crime[\"UCR_PART\"]) "
      ],
      "id": "1f6510af"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06afa6f2"
      },
      "outputs": [],
      "source": [
        "# here we work on locataion . we add two columns for location_x and location_y \n",
        "# first we\n",
        "def remove(text) :\n",
        "    xandy = []\n",
        "    x = text\n",
        "    y = re.findall(r'\\d+\\.\\d', x)\n",
        "    for i in y:\n",
        "        xandy.append(i)\n",
        "    return xandy[0] , xandy[1]"
      ],
      "id": "06afa6f2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "791234bd"
      },
      "outputs": [],
      "source": [
        "x = []\n",
        "y = []\n",
        "\n",
        "for item in range(len(Crime)) :\n",
        "  if not pd.isna(Crime['Location'][item]) :\n",
        "        x_location , y_location = remove(Crime['Location'][item])\n",
        "        x.append(x_location)\n",
        "        y.append(y_location)\n",
        "       \n",
        "new_dataset[\"X_Location\"] = pd.DataFrame(x)\n",
        "new_dataset[\"Y_Location\"] = pd.DataFrame(y)                "
      ],
      "id": "791234bd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2bc1311"
      },
      "source": [
        "in this section we have to add other columns that dont need preprocessing \n",
        "here the features are \"OFFENSE_CODE_GROUP\" , \"STREET\" , \"Lat\" , \"Long\""
      ],
      "id": "b2bc1311"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b44254d"
      },
      "outputs": [],
      "source": [
        "# we have to convert \"OFFENSE_CODE_GROUP\" to numerical . in the next step we wnt use thease data in our ML models\n",
        "\n",
        "new_dataset[\"OFFENSE_CODE_GROUP\"] = LabelEncoder().fit_transform(Crime[\"OFFENSE_CODE_GROUP\"]) \n",
        "\n",
        "new_dataset[\"STREET\"] = LabelEncoder().fit_transform(Crime[\"STREET\"]) \n",
        "\n",
        "new_dataset[\"Lat\"] = Crime[\"Lat\"]\n",
        "\n",
        "new_dataset[\"Long\"] = Crime[\"Long\"]\n"
      ],
      "id": "8b44254d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30691052"
      },
      "outputs": [],
      "source": [
        "# in this section we are going to fill in the null sectors of \"SHOOTING\" feature . we have approimately 324000 data \n",
        "# but only nearly 1700 of SHOOTING sectors have valeu , however all of them are \"Y\"  so maybe the outhers are \"N\" ,so \n",
        "# we consider them as \"N\" .\n",
        "for item in range(len(Crime)) :\n",
        "    if pd.isna(Crime[\"SHOOTING\"][item]) :\n",
        "        Crime[\"SHOOTING\"].iloc[item] = \"N\"\n",
        "new_dataset[\"SHOOTING\"] = LabelEncoder().fit_transform(Crime[\"SHOOTING\"]) \n",
        "\n",
        "# now we have to transform \"Y\" and \"N\" to 0 and 1 respectively by LabelEncoding in sklearn library .\n"
      ],
      "id": "30691052"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJsQS62N8rHh"
      },
      "outputs": [],
      "source": [
        "new_dataset[\"OFFENSE_CODE_GROUP_WITHOUT_ENCODING\"] = Crime[\"OFFENSE_CODE_GROUP\"]\n",
        "new_dataset[\"STREET_WITHOUT_ENCODING\"] = Crime[\"STREET\"]"
      ],
      "id": "QJsQS62N8rHh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oURvx4s87vaB"
      },
      "source": [
        "# Data Cleaning\n",
        "here we are going to drop null data ."
      ],
      "id": "oURvx4s87vaB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5g5wJu07jQC"
      },
      "outputs": [],
      "source": [
        "final_dataset = new_dataset.dropna(subset= [\"OFFENSE_CODE_GROUP\"  , \"MONTH\" , \"DAY_OF_WEEK\" , \"HOUR\" , \"X_Location\", \"Y_Location\" ]) "
      ],
      "id": "-5g5wJu07jQC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3a94770"
      },
      "outputs": [],
      "source": [
        "final_dataset.to_csv(\"new_dataset.csv\")"
      ],
      "id": "e3a94770"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28097a6a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "duplicated = Crime[\"OFFENSE_CODE_GROUP\"].duplicated()\n",
        "data = [Crime[\"OFFENSE_CODE_GROUP\"][i] for i in range(len(Crime[\"OFFENSE_CODE_GROUP\"])) if duplicated[i] == False ]\n",
        "\n",
        "data"
      ],
      "id": "28097a6a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cd3ac90"
      },
      "outputs": [],
      "source": [
        "duplicated_day = Crime[\"DAY_OF_WEEK\"].duplicated()\n",
        "days = [Crime[\"DAY_OF_WEEK\"][i] for i in range(len(Crime[\"DAY_OF_WEEK\"])) if duplicated_day[i] == False ]\n",
        "days"
      ],
      "id": "7cd3ac90"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMHZCYI1Cu43"
      },
      "outputs": [],
      "source": [
        "Crime.info()"
      ],
      "id": "dMHZCYI1Cu43"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}